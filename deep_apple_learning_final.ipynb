{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deep-apple-learning_final",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gsrflo/deep-apple-learning/blob/dev/deep_apple_learning_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmIOkV65f5S0"
      },
      "source": [
        "### Import Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1f7bWu9lJtX"
      },
      "source": [
        "#If pytorch lightning not installed:\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "# Module for Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Module for Importing Images\n",
        "from PIL import Image \n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import collections\n",
        "\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA3MZRzNHclY"
      },
      "source": [
        "### Import Drive Content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXPZ3dc6W3kp",
        "outputId": "edb37f28-9ddf-424e-c96b-c00d7f4b9810"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTj1RldNHgSQ"
      },
      "source": [
        "### Definition of Dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFxwfpOxHcAk"
      },
      "source": [
        "class AppleDataset(Dataset):\n",
        "    def __init__(self, data_path, img_type, set_type):\n",
        "        #choose between rgb or ir data: train, validate or test\n",
        "        self.img_type = img_type # 'rgb' or 'ir'\n",
        "        self.set_type = set_type # 'train', 'validate', 'test', 'evaluation'\n",
        "\n",
        "        #set paths to training / validation / test folders:\n",
        "        self.data_path = data_path\n",
        "        self.target_path = os.path.join(data_path, self.img_type, self.set_type)\n",
        "\n",
        "        #get labels and filenames from labels.txt:\n",
        "        labels_path = os.path.join(data_path, 'labels_' + self.img_type + '.txt')\n",
        "        labels_txt = open(labels_path, 'r')\n",
        "        labels_txt = labels_txt.readlines()\n",
        "        label = list()\n",
        "        samplename = list()\n",
        "        for i in range(len(labels_txt)):\n",
        "            txtrow = labels_txt[i].rstrip().split(' ')\n",
        "            if(len(txtrow) > 1):    #because of empty row between apples in txt\n",
        "              samplename.append(txtrow[0] + '.jpg')\n",
        "              label.append(txtrow[1])\n",
        "\n",
        "        #transforming label from string to int:\n",
        "        label =  [int(i) for i in label]\n",
        "\n",
        "        #combine path, label and name of sample:\n",
        "        img_list = sorted(os.listdir(self.target_path))\n",
        "        self.img_path_label = list()\n",
        "        for fp in img_list:\n",
        "            img_label = label[samplename.index(fp)]\n",
        "            full_fp = os.path.join(self.target_path, fp)\n",
        "            self.img_path_label.append((full_fp, img_label, fp))\n",
        "\n",
        "        #data augmentation:\n",
        "        self.tensor_transform = torchvision.transforms.ToTensor()\n",
        "        \n",
        "        self.random_flip = torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
        "        self.random_affine = torchvision.transforms.RandomAffine(20)\n",
        "            \n",
        "        self.transform = torchvision.transforms.Compose([self.tensor_transform, self.random_flip,self.random_affine])\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #get filepath of sample image, label, and name:\n",
        "        (fp, label, name) = self.img_path_label[idx]\n",
        "\n",
        "        #sample image:\n",
        "        img = Image.open(fp)\n",
        "        \n",
        "        #data augmentation only for train set:\n",
        "        if self.set_type == 'train':\n",
        "          sample = self.transform(img)\n",
        "        else:\n",
        "          sample = self.tensor_transform(img) \n",
        "\n",
        "        #normalization:\n",
        "        mean = torch.mean(sample, axis=(1,2), keepdims=True)\n",
        "        std = torch.std(sample, axis=(1,2), keepdims=True)\n",
        "        sample = (sample - mean) / (std + 1e-11)\n",
        "\n",
        "        return sample, label, name, self.tensor_transform(img)      #name and img is only used for plotting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPFSRXfal8Dl"
      },
      "source": [
        "### Set DataSet and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJLrwTOxI127"
      },
      "source": [
        "#path of dataset:\n",
        "data_path = '/content/drive/MyDrive/deep-apple-learning/dataset/' \n",
        "\n",
        "#path of model (for manual storing and checkpoints):\n",
        "model_dir = '/content/drive/MyDrive/deep-apple-learning/checkpoint_models/' \n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "#infrared:\n",
        "train_dataset_ir = AppleDataset(data_path, 'ir', 'train')\n",
        "train_dataloader_ir = DataLoader(train_dataset_ir, batch_size=batch_size, shuffle=True)\n",
        "val_dataset_ir = AppleDataset(data_path, 'ir', 'validate')\n",
        "val_dataloader_ir = DataLoader(val_dataset_ir, batch_size=52, shuffle=False)      #number of val images = 52\n",
        "test_dataset_ir = AppleDataset(data_path, 'ir', 'test')\n",
        "test_dataloader_ir = DataLoader(test_dataset_ir, batch_size=55, shuffle=False)    #number of test images = 55\n",
        "\n",
        "#rgb:\n",
        "train_dataset_rgb = AppleDataset(data_path, 'rgb', 'train')\n",
        "train_dataloader_rgb = DataLoader(train_dataset_rgb, batch_size=batch_size, shuffle=True)\n",
        "val_dataset_rgb = AppleDataset(data_path, 'rgb', 'validate')\n",
        "val_dataloader_rgb = DataLoader(val_dataset_rgb, batch_size=64, shuffle=False)      #number of val images = 64\n",
        "test_dataset_rgb = AppleDataset(data_path, 'rgb', 'test')\n",
        "test_dataloader_rgb = DataLoader(test_dataset_rgb, batch_size=55, shuffle=False)    #number of test images = 55"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6s35e4ymD-M"
      },
      "source": [
        "### Take and show a sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI2z7aqgMcUJ",
        "outputId": "19e4a226-15e1-4cb4-b152-1fdbc189663f"
      },
      "source": [
        "#infrared sample:\r\n",
        "sample_train_ir, label_train_ir, name_train_ir, img_train_ir = next(iter(train_dataloader_ir))\r\n",
        "\r\n",
        "#rgb samples:\r\n",
        "sample_train_rgb, label_train_rgb, name_train_rgb, img_train_rgb = next(iter(train_dataloader_rgb))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n",
            "  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rsIBq2rIxA_"
      },
      "source": [
        "#choose IR sample:\n",
        "sample = img_train_ir\n",
        "label = label_train_ir\n",
        "name = name_train_ir\n",
        "\n",
        "plot_idx = np.random.randint(0, batch_size)\n",
        "img = sample[plot_idx]\n",
        "plt.imshow(img.repeat(3,1,1).permute(1,2,0))\n",
        "#plt.imshow(img[:][:][0], cmap='gray')\n",
        "plt.title('Filename: ' + name[plot_idx] + ', Label: ' + str(label[plot_idx].detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8uqeiYVXw6u"
      },
      "source": [
        "#choose RGB sample:\r\n",
        "sample = img_train_rgb\r\n",
        "label = label_train_rgb\r\n",
        "name = name_train_rgb\r\n",
        "\r\n",
        "plot_idx = np.random.randint(0, batch_size)\r\n",
        "img = sample[plot_idx]\r\n",
        "\r\n",
        "plt.imshow(img.permute(1,2,0))\r\n",
        "plt.title('Filename: ' + name[plot_idx] + ', Label: ' + str(label[plot_idx].detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYiku-SxHnA6"
      },
      "source": [
        "### Choose your device (GPU or CPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k44xmCOHHmF4",
        "outputId": "f3edd596-5514-4f6b-af52-ea06a2e14c28"
      },
      "source": [
        "#device = 'cpu'\n",
        "device = 'cuda'\n",
        "print('Current Device : {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Device : cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEjuX7lWIYlb"
      },
      "source": [
        "### Define models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx-fxCW5JD_w"
      },
      "source": [
        "class IR_Model(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(IR_Model, self).__init__()\n",
        "        \n",
        "        self.cnn = nn.Sequential(collections.OrderedDict([\n",
        "                  ('conv1', nn.Conv2d(in_channels=1, out_channels = 16, kernel_size = 3, stride = 1, padding = 1)),\n",
        "                  ('bn1', nn.BatchNorm2d(16)),\n",
        "                  ('elu1', nn.ELU()),\n",
        "                  ('maxpool1', nn.MaxPool2d(kernel_size = 2, stride = 2)),\n",
        "                  ('conv2', nn.Conv2d(in_channels=16, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)),\n",
        "                  ('bn2', nn.BatchNorm2d(64)),\n",
        "                  ('elu2', nn.ELU()),\n",
        "                  ('maxpool2', nn.MaxPool2d(kernel_size = 2, stride = 2)),\n",
        "                  ('conv3', nn.Conv2d(in_channels=64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)),\n",
        "                  ('bn3', nn.BatchNorm2d(128)),\n",
        "                  ('elu3', nn.ELU()),\n",
        "                  ('maxpool3', nn.MaxPool2d(kernel_size = 2, stride = 2)),\n",
        "                  ('conv4', nn.Conv2d(in_channels=128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)),\n",
        "                  ('bn4', nn.BatchNorm2d(256)),\n",
        "                  ('elu4', nn.ELU()),\n",
        "                  ('avgpool', nn.AvgPool2d(kernel_size=15)),\n",
        "                  ('flatten', nn.Flatten())\n",
        "                  ]))\n",
        "        \n",
        "        self.fc1 = nn.Linear(256,64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64,16)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(16,1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = self.cnn(img.to(device))\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.sig(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        batch, label, _ , _  = train_batch\n",
        "        pred = self(batch)\n",
        "        \n",
        "        #loss function:\n",
        "        loss = torch.tensor(0., requires_grad=True).to(device)\n",
        "        for idx, samplepred in enumerate(pred):\n",
        "            if label[idx]==1:\n",
        "              loss = loss - torch.log(samplepred)\n",
        "            elif label[idx]==0:  \n",
        "              loss = loss - torch.log(1. - samplepred)\n",
        "        self.log('train_loss', loss)\n",
        "\n",
        "        #accuracy:\n",
        "        pred_ir = (pred > 0.5)\n",
        "        acc = torch.sum(pred_ir.transpose(0,1) == label)/len(pred_ir)\n",
        "        self.log('train_accuracy', acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        sample , label, _, _   = val_batch\n",
        "        pred = self(sample)    \n",
        "\n",
        "        #loss function:\n",
        "        loss = torch.tensor(0. , requires_grad=True).to(device)\n",
        "        for idx, samplepred in enumerate(pred):\n",
        "            if label[idx]==1:\n",
        "              loss = loss - torch.log(samplepred)\n",
        "            elif label[idx]==0: \n",
        "              loss = loss - torch.log(1. - samplepred)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "\n",
        "        #accuracy:\n",
        "        pred_ir = (pred > 0.5)\n",
        "        acc = torch.sum(pred_ir.transpose(0,1) == label)/len(pred_ir)\n",
        "        self.log('val_accuracy', acc,prog_bar=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqcx1BjiwCBh"
      },
      "source": [
        "class RGB_Model(pl.LightningModule):\r\n",
        "    def __init__(self):\r\n",
        "        super(RGB_Model, self).__init__()\r\n",
        "\r\n",
        "        self.cnn = nn.Sequential(collections.OrderedDict([\r\n",
        "                  ('conv1', nn.Conv2d(in_channels=3, out_channels = 16, kernel_size = 3, stride = 1, padding = 1)),\r\n",
        "                  ('bn1', nn.BatchNorm2d(16)),\r\n",
        "                  ('elu1', nn.ELU()),\r\n",
        "                  ('maxpool1', nn.MaxPool2d(kernel_size = 2, stride = 2)),\r\n",
        "                  ('conv2', nn.Conv2d(in_channels=16, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)),\r\n",
        "                  ('bn2', nn.BatchNorm2d(64)),\r\n",
        "                  ('elu2', nn.ELU()),\r\n",
        "                  ('maxpool2', nn.MaxPool2d(kernel_size = 2, stride = 2)),\r\n",
        "                  ('conv3', nn.Conv2d(in_channels=64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)),\r\n",
        "                  ('bn3', nn.BatchNorm2d(128)),\r\n",
        "                  ('elu3', nn.ELU()),\r\n",
        "                  ('maxpool3', nn.MaxPool2d(kernel_size = 2, stride = 2)),\r\n",
        "                  ('conv4', nn.Conv2d(in_channels=128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)),\r\n",
        "                  ('bn4', nn.BatchNorm2d(256)),\r\n",
        "                  ('elu4', nn.ELU()),\r\n",
        "                  ('avgpool', nn.AvgPool2d(kernel_size=15)),\r\n",
        "                  ('flatten', nn.Flatten())\r\n",
        "                  ]))\r\n",
        "        \r\n",
        "        self.fc1 = nn.Linear(256,64)\r\n",
        "        self.relu1 = nn.ReLU()\r\n",
        "        self.fc2 = nn.Linear(64,16)\r\n",
        "        self.relu2 = nn.ReLU()\r\n",
        "        self.fc3 = nn.Linear(16,1)\r\n",
        "        self.sig = nn.Sigmoid()\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p=0)\r\n",
        "\r\n",
        "    def forward(self, img):\r\n",
        "        out = self.cnn(img.to(device))\r\n",
        "        out = self.fc1(out)\r\n",
        "        out = self.relu1(out)\r\n",
        "        out = self.fc2(out)\r\n",
        "        out = self.relu2(out)\r\n",
        "        out = self.fc3(out)\r\n",
        "        out = self.sig(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n",
        "        return optimizer\r\n",
        "\r\n",
        "    def training_step(self, train_batch, batch_idx):\r\n",
        "        batch, label, _ , _  = train_batch\r\n",
        "        pred = self(batch)\r\n",
        "\r\n",
        "        #loss function:\r\n",
        "        loss = torch.tensor(0., requires_grad=True).to(device)\r\n",
        "        for idx, samplepred in enumerate(pred):\r\n",
        "            if label[idx]==1:\r\n",
        "              loss = loss - torch.log(samplepred)\r\n",
        "            elif label[idx]==0: \r\n",
        "              loss = loss - torch.log(1. - samplepred)\r\n",
        "        self.log('train_loss', loss)\r\n",
        "\r\n",
        "        #accuracy:\r\n",
        "        pred_rgb = (pred > 0.5)\r\n",
        "        acc = torch.sum(pred_rgb.transpose(0,1) == label)/len(pred_rgb)\r\n",
        "        self.log('train_accuracy', acc, prog_bar=True)\r\n",
        "\r\n",
        "        return loss\r\n",
        "      \r\n",
        "    def validation_step(self, val_batch, batch_idx):\r\n",
        "        sample, label, _ , _  = val_batch\r\n",
        "        pred = self(sample)    \r\n",
        "\r\n",
        "        #loss function:\r\n",
        "        loss = torch.tensor(0. , requires_grad=True).to(device)\r\n",
        "        for idx, samplepred in enumerate(pred):\r\n",
        "            if label[idx]==1:\r\n",
        "              loss = loss - torch.log(samplepred)\r\n",
        "            elif label[idx]==0:  \r\n",
        "              loss = loss - torch.log(1. - samplepred)\r\n",
        "        self.log('val_loss', loss, prog_bar=True)\r\n",
        "\r\n",
        "        #accuracy:\r\n",
        "        pred_rgb = (pred > 0.5)\r\n",
        "        acc = torch.sum(pred_rgb.transpose(0,1) == label)/len(pred_rgb)\r\n",
        "        self.log('val_accuracy', acc, prog_bar=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdKQ5y4hX3Me"
      },
      "source": [
        "class IR_Model_pretrained(pl.LightningModule):\r\n",
        "    def __init__(self):\r\n",
        "        super(IR_Model_pretrained, self).__init__()\r\n",
        "\r\n",
        "        #choose pretrained model:\r\n",
        "        #self.backbone = torchvision.models.vgg19_bn(pretrained=True)\r\n",
        "        self.backbone = torchvision.models.resnet50(pretrained=True)\r\n",
        "\r\n",
        "        #fix initial layers of pretrained model:\r\n",
        "        for p in list(self.backbone.children())[:-3]:\r\n",
        "            p.requires_grad = False\r\n",
        "\r\n",
        "        # get the structure until the Fully Connected Layer\r\n",
        "        modules = list(self.backbone.children())[:-1]\r\n",
        "        self.backbone = nn.Sequential(*modules)\r\n",
        "\r\n",
        "        #create new network:\r\n",
        "        self.fc1 = nn.Linear(2048,512)\r\n",
        "        self.relu1 = nn.ReLU()\r\n",
        "        self.fc2 = nn.Linear(512,128)\r\n",
        "        self.relu2 = nn.ReLU()\r\n",
        "        self.fc3 = nn.Linear(128,16)\r\n",
        "        self.relu3 = nn.ReLU()\r\n",
        "        self.fc4 = nn.Linear(16,1)\r\n",
        "        self.sig = nn.Sigmoid()\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p=0.2)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, img):\r\n",
        "        #Convert grayscale IR image to RGB image:\r\n",
        "        img = img.repeat(1, 3, 1, 1)\r\n",
        "\r\n",
        "        out = self.backbone(img.to(device))\r\n",
        "        out = self.fc1(out.view(out.size(0), -1))\r\n",
        "        out = self.relu1(out)\r\n",
        "        out = self.fc2(out)\r\n",
        "        out = self.relu2(out)\r\n",
        "        out = self.fc3(out)\r\n",
        "        out = self.relu3(out)\r\n",
        "        out = self.fc4(out)\r\n",
        "        out = self.sig(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n",
        "        return optimizer\r\n",
        "\r\n",
        "    def training_step(self, train_batch, batch_idx):\r\n",
        "        batch, label, _ , _  = train_batch\r\n",
        "\r\n",
        "        #Convert grayscale IR image to RGB image:\r\n",
        "        batch = batch.repeat(1, 3, 1, 1)\r\n",
        "\r\n",
        "        out = self.backbone(batch)\r\n",
        "        out = self.dropout(self.fc1(out.view(out.size(0), -1)))\r\n",
        "        out = self.relu1(out)\r\n",
        "        out = self.dropout(self.fc2(out))\r\n",
        "        out = self.relu2(out)\r\n",
        "        out = self.dropout(self.fc3(out))\r\n",
        "        out = self.relu3(out)\r\n",
        "        out = self.dropout(self.fc4(out))\r\n",
        "        pred = self.sig(out)\r\n",
        "\r\n",
        "        #loss function:\r\n",
        "        loss = torch.tensor(0., requires_grad=True).to(device)\r\n",
        "        for idx, samplepred in enumerate(pred):\r\n",
        "            if label[idx]==1:\r\n",
        "              loss = loss - torch.log(samplepred)\r\n",
        "            elif label[idx]==0:  \r\n",
        "              loss = loss - torch.log(1. - samplepred)\r\n",
        "        self.log('train_loss', loss)\r\n",
        "\r\n",
        "        #accuracy:\r\n",
        "        pred_ir = (pred > 0.5)\r\n",
        "        acc = torch.sum(pred_ir.transpose(0,1) == label)/len(pred_ir)\r\n",
        "        self.log('train_accuracy', acc, prog_bar=True)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def validation_step(self, val_batch, batch_idx):\r\n",
        "        sample, label, _ , _   = val_batch\r\n",
        "        pred = self(sample)    \r\n",
        "\r\n",
        "        #loss function:\r\n",
        "        loss = torch.tensor(0. , requires_grad=True).to(device)\r\n",
        "        for idx, samplepred in enumerate(pred):\r\n",
        "            if label[idx]==1:\r\n",
        "              loss = loss - torch.log(samplepred)\r\n",
        "            elif label[idx]==0: \r\n",
        "              loss = loss - torch.log(1. - samplepred)\r\n",
        "        self.log('val_loss', loss, prog_bar=True)\r\n",
        "\r\n",
        "        #accuracy:\r\n",
        "        pred_ir = (pred > 0.5)\r\n",
        "        acc = torch.sum(pred_ir.transpose(0,1) == label)/len(pred_ir)\r\n",
        "        self.log('val_accuracy', acc, prog_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYItKYfYP41I"
      },
      "source": [
        "class RGB_Model_pretrained(pl.LightningModule):\r\n",
        "    def __init__(self):\r\n",
        "        super(RGB_Model_pretrained, self).__init__()\r\n",
        "\r\n",
        "        #choose pretrained model:\r\n",
        "        #self.backbone = torchvision.models.vgg19_bn(pretrained=True)\r\n",
        "        self.backbone = torchvision.models.resnet50(pretrained=True)\r\n",
        "\r\n",
        "        #fix initial layers of pretrained model:\r\n",
        "        for p in list(self.backbone.children())[:-3]:\r\n",
        "            p.requires_grad = False\r\n",
        "\r\n",
        "        # get the structure until the Fully Connected Layer\r\n",
        "        modules = list(self.backbone.children())[:-1]\r\n",
        "        self.backbone = nn.Sequential(*modules)\r\n",
        "\r\n",
        "        #create new network:\r\n",
        "        self.fc1 = nn.Linear(2048,512)\r\n",
        "        self.relu1 = nn.ReLU()\r\n",
        "        self.fc2 = nn.Linear(512,128)\r\n",
        "        self.relu2 = nn.ReLU()\r\n",
        "        self.fc3 = nn.Linear(128,16)\r\n",
        "        self.relu3 = nn.ReLU()\r\n",
        "        self.fc4 = nn.Linear(16,1)\r\n",
        "        self.sig = nn.Sigmoid()\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p=0.2)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, img):\r\n",
        "        out = self.backbone(img.to(device))\r\n",
        "        out = self.fc1(out.view(out.size(0), -1))\r\n",
        "        out = self.relu1(out)\r\n",
        "        out = self.fc2(out)\r\n",
        "        out = self.relu2(out)\r\n",
        "        out = self.fc3(out)\r\n",
        "        out = self.relu3(out)\r\n",
        "        out = self.fc4(out)\r\n",
        "        out = self.sig(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n",
        "        return optimizer\r\n",
        "\r\n",
        "    def training_step(self, train_batch, batch_idx):\r\n",
        "        batch, label, _ , _ = train_batch\r\n",
        "        out = self.backbone(batch)\r\n",
        "        out = self.dropout(self.fc1(out.view(out.size(0), -1)))\r\n",
        "        out = self.relu1(out)\r\n",
        "        out = self.dropout(self.fc2(out))\r\n",
        "        out = self.relu2(out)\r\n",
        "        out = self.dropout(self.fc3(out))\r\n",
        "        out = self.relu3(out)\r\n",
        "        out = self.dropout(self.fc4(out))\r\n",
        "        pred = self.sig(out)\r\n",
        "\r\n",
        "        #loss function:\r\n",
        "        loss = torch.tensor(0., requires_grad=True).to(device)\r\n",
        "        for idx, samplepred in enumerate(pred):\r\n",
        "            if label[idx]==1:\r\n",
        "              loss = loss - torch.log(samplepred)\r\n",
        "            elif label[idx]==0:  \r\n",
        "              loss = loss - torch.log(1. - samplepred)\r\n",
        "        self.log('train_loss', loss)\r\n",
        "\r\n",
        "        #accuracy:\r\n",
        "        pred_ir = (pred > 0.5)\r\n",
        "        acc = torch.sum(pred_ir.transpose(0,1) == label)/len(pred_ir)\r\n",
        "        self.log('train_accuracy', acc, prog_bar=True)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def validation_step(self, val_batch, batch_idx):\r\n",
        "        sample, label, _ , _  = val_batch\r\n",
        "        pred = self(sample)    \r\n",
        "\r\n",
        "        #loss function:\r\n",
        "        loss = torch.tensor(0. , requires_grad=True).to(device)\r\n",
        "        for idx, samplepred in enumerate(pred):\r\n",
        "            if label[idx]==1:\r\n",
        "              loss = loss - torch.log(samplepred)\r\n",
        "            elif label[idx]==0: \r\n",
        "              loss = loss - torch.log(1. - samplepred)\r\n",
        "        self.log('val_loss', loss, prog_bar=True)\r\n",
        "\r\n",
        "        #accuracy:\r\n",
        "        pred_ir = (pred > 0.5)\r\n",
        "        acc = torch.sum(pred_ir.transpose(0,1) == label)/len(pred_ir)\r\n",
        "        self.log('val_accuracy', acc, prog_bar=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4hjFl4-IsqO"
      },
      "source": [
        "### Create model and train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWtw-y-MKuv1"
      },
      "source": [
        "#IR Model (choose selfmade or pretrained):\n",
        "\n",
        "# model_ir = IR_Model()\n",
        "model_ir = IR_Model_pretrained()\n",
        "\n",
        "model_ir = model_ir.to(device)\n",
        "\n",
        "\n",
        "\n",
        "#RGB Model (choose selfmade or pretrained):\n",
        "\n",
        "# model_rgb = RGB_Model()\n",
        "model_rgb = RGB_Model_pretrained()\n",
        "\n",
        "model_rgb = model_rgb.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luBeVvgnxCj-"
      },
      "source": [
        "#Train IR Model:\r\n",
        "\r\n",
        "# Saves checkpoints to 'my/path/' at every epoch\r\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=model_dir + 'ir/')\r\n",
        "\r\n",
        "trainer_ir = pl.Trainer(gpus=-1, precision=16, progress_bar_refresh_rate = 20, max_epochs=150, profiler = True, callbacks=[checkpoint_callback])\r\n",
        "trainer_ir.fit(model_ir, train_dataloader_ir, val_dataloader_ir)\r\n",
        "\r\n",
        "\r\n",
        "# Save epoch and val_loss in name: saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\r\n",
        "checkpoint_callback = ModelCheckpoint(\r\n",
        "     monitor='val_loss',\r\n",
        "     dirpath=model_dir + 'ir/',\r\n",
        "     filename='sample-ir-{epoch:02d}-{val_loss:.2f}'\r\n",
        ")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGPq9vKwxfwi"
      },
      "source": [
        "#Train RGB Model:\r\n",
        "\r\n",
        "# Saves checkpoints to 'my/path/' at every epoch\r\n",
        "checkpoint_callback = ModelCheckpoint(dirpath=model_dir + 'rgb/')\r\n",
        "\r\n",
        "trainer_rgb = pl.Trainer(gpus=-1, precision=16, progress_bar_refresh_rate = 20, max_epochs=150, profiler = True, callbacks=[checkpoint_callback])\r\n",
        "trainer_rgb.fit(model_rgb, train_dataloader_rgb, val_dataloader_rgb)\r\n",
        "\r\n",
        "# Save epoch and val_loss in name: saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\r\n",
        "checkpoint_callback = ModelCheckpoint(\r\n",
        "     monitor='val_loss',\r\n",
        "     dirpath=model_dir + 'rgb/',\r\n",
        "     filename='sample-rgb-{epoch:02d}-{val_loss:.2f}'\r\n",
        ")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQujqRy00qHl"
      },
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4lNvY0AyAvV"
      },
      "source": [
        "### Load Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ynbXX1bTSfj"
      },
      "source": [
        "#Load Checkpoint Model\r\n",
        "\r\n",
        "#loading checkpoint model (from folder 'checkpoint_models'): flag_statedict = 0\r\n",
        "#loading state-dict model (from folder 'models'): flag_statedict = 1\r\n",
        "\r\n",
        "flag_statedict_ir = 0\r\n",
        "flag_statedict_rgb = 0\r\n",
        "\r\n",
        "\r\n",
        "#IR Model:\r\n",
        "if flag_statedict_ir:\r\n",
        "  #For manually saved state-dict models:\r\n",
        "  model_path_ir = model_dir + 'ir/model_ir.pth'\r\n",
        "  model_ir = IR_Model_pretrained()    #IR_Model() or IR_Model_pretrained()\r\n",
        "  model_ir.load_state_dict(torch.load(model_path_ir))\r\n",
        "  model_ir = model_ir.to(device)\r\n",
        "\r\n",
        "else:\r\n",
        "  #For pytorch lightning checkpoint models: \r\n",
        "  model_path_ir = model_dir + 'ir/ir_pretrained.ckpt' \r\n",
        "  model_ir = IR_Model_pretrained.load_from_checkpoint(checkpoint_path=model_path_ir).to(device)     #use this line if the checkpoint model is pretrained\r\n",
        "  #model_ir = IR_Model.load_from_checkpoint(checkpoint_path=model_path_ir).to(device)               #use this line if the checkpoint model is not the pretrained one.\r\n",
        "\r\n",
        "#RGB Model:\r\n",
        "if flag_statedict_rgb:\r\n",
        "  #For manually saved state-dict models:\r\n",
        "  model_path_rgb = model_dir + 'rgb/model_rgb.pth'\r\n",
        "  model_rgb = RGB_Model_pretrained()    #RGB_Model() or RGB_Model_pretrained()\r\n",
        "  model_rgb.load_state_dict(torch.load(model_path_rgb))\r\n",
        "  model_rgb = model_rgb.to(device)\r\n",
        "\r\n",
        "else:\r\n",
        "  #For pytorch lightning checkpoint models:\r\n",
        "  model_path_rgb = model_dir + 'rgb/rgb_pretrained.ckpt' \r\n",
        "  model_rgb = RGB_Model_pretrained.load_from_checkpoint(checkpoint_path=model_path_rgb).to(device)  #use this line if the checkpoint model is pretrained\r\n",
        "  #model_rgb = RGB_Model.load_from_checkpoint(checkpoint_path=model_path_rgb).to(device)            #use this line if the checkpoint model is not the pretrained one.\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbsMrUjXAq-t"
      },
      "source": [
        "### Test Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv_Y4yNJw8k2"
      },
      "source": [
        "#Test function:\r\n",
        "def test_model(model, sample, label):\r\n",
        "\r\n",
        "    label = [int(i) for i in label.detach().numpy()]     # like this, it is easier to compare ground truth and prediction when printing both\r\n",
        "    model = model.to(device)\r\n",
        "\r\n",
        "    ## Start time:\r\n",
        "    t0 = time.clock()\r\n",
        "\r\n",
        "    pred0 = model(sample)\r\n",
        "    pred = (pred0 > 0.5)\r\n",
        "    pred = [int(i) for i in pred]\r\n",
        "    acc = np.sum(np.equal(pred, label))/len(pred)\r\n",
        "\r\n",
        "    ## End time:\r\n",
        "    t  = time.clock() - t0\r\n",
        "\r\n",
        "    print('Prediction  : ' + str(pred))\r\n",
        "    print('Ground Truth: ' + str(label))\r\n",
        "    print('Accuracy    : ' + str(acc))\r\n",
        "    print('Time elapsed: ' + str(t))\r\n",
        "    print(' ')\r\n",
        "\r\n",
        "    # Calculate Confusion Matrix\r\n",
        "    target = torch.tensor(label)\r\n",
        "    preds = torch.tensor(pred)\r\n",
        "    confmat = pl.metrics.ConfusionMatrix(num_classes=2, normalize='true')\r\n",
        "    confusion_matrix = confmat(preds, target)\r\n",
        "\r\n",
        "    # Plot Confusion Matrix\r\n",
        "    df_cm = pd.DataFrame((np.array(confusion_matrix)), range(2), range(2))\r\n",
        "    plt.figure(figsize=(10,7))\r\n",
        "    sn.set(font_scale=1.4) \r\n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\r\n",
        "    print('Confusion Matrix:')\r\n",
        "    plt.xlabel('Prediction')\r\n",
        "    plt.ylabel('Ground Truth')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    return pred\r\n",
        "\r\n",
        "#Test function for combined prediction:\r\n",
        "def test_combined(model_ir, model_rgb, sample_ir, sample_rgb, label_ir, label_rgb):\r\n",
        "    ##  IR and RGB imagery has same order\r\n",
        "\r\n",
        "    label_ir = [int(i) for i in label_ir.detach().numpy()]\r\n",
        "    label_rgb = [int(i) for i in label_rgb.detach().numpy()]\r\n",
        "    label_combined = (label_rgb and label_ir) # apple only sound when both vectors predict\r\n",
        "\r\n",
        "    ## Start time:\r\n",
        "    t0 = time.clock()\r\n",
        "\r\n",
        "    #IR prediction:\r\n",
        "    pred_ir0 = model_ir(sample_ir)\r\n",
        "    pred_ir = (pred_ir0 > 0.5)\r\n",
        "    pred_ir = [int(i) for i in pred_ir]\r\n",
        "\r\n",
        "    #RGB prediction:\r\n",
        "    pred_rgb0 = model_rgb(sample_rgb)\r\n",
        "    pred_rgb = (pred_rgb0 > 0.5)\r\n",
        "    pred_rgb = [int(i) for i in pred_rgb]\r\n",
        "\r\n",
        "    ## Combined prediction:\r\n",
        "\r\n",
        "    # Compare confidence of both predictions and label as '1' if one prediction is above the threshold\r\n",
        "    threshold = 0.5\r\n",
        "    pred_combined = []\r\n",
        "\r\n",
        "    # apple is sound --> '1'\r\n",
        "    for i in range(len(pred_ir0)):\r\n",
        "      if pred_ir[i] == 1 and pred_rgb[i] == 1:\r\n",
        "        pred_combined.append(1)\r\n",
        "      elif pred_ir[i] == 0 and pred_rgb[i] == 0:\r\n",
        "        pred_combined.append(0)\r\n",
        "      elif pred_ir0[i] < 1 - threshold or pred_rgb0[i] < 1 - threshold:\r\n",
        "        pred_combined.append(0)\r\n",
        "      else:\r\n",
        "        pred_combined.append(1)\r\n",
        "\r\n",
        "    acc_combined = np.sum(np.equal(pred_combined, label_combined))/len(pred_combined)\r\n",
        "    \r\n",
        "    ## End time:\r\n",
        "    t  = time.clock() - t0\r\n",
        "\r\n",
        "    print('Prediction  : ' + str(pred_combined))\r\n",
        "    print('Ground Truth: ' + str(label_combined))\r\n",
        "    print('Accuracy    : ' + str(acc_combined))\r\n",
        "    print('Time elapsed: ' + str(t))\r\n",
        "    print(' ')\r\n",
        "\r\n",
        "    # Calculate Confusion Matrix\r\n",
        "    target = torch.tensor(label_combined)\r\n",
        "    preds = torch.tensor(pred_combined)\r\n",
        "    confmat = pl.metrics.ConfusionMatrix(num_classes=2, normalize='true')\r\n",
        "    confusion_matrix = confmat(preds, target)\r\n",
        "\r\n",
        "    # Plot Confusion Matrix\r\n",
        "    df_cm = pd.DataFrame((np.array(confusion_matrix)), range(2), range(2))\r\n",
        "    plt.figure(figsize=(10,7))\r\n",
        "    sn.set(font_scale=1.4) \r\n",
        "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\r\n",
        "    print('Confusion Matrix:')\r\n",
        "    plt.xlabel('Prediction')\r\n",
        "    plt.ylabel('Ground Truth')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    return pred_combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBP7jztfhFjQ"
      },
      "source": [
        "## Get test samples:\r\n",
        "\r\n",
        "#infrared sample:\r\n",
        "sample_ir, label_ir, _ , _ = next(iter(test_dataloader_ir))\r\n",
        "\r\n",
        "#rgb samples:\r\n",
        "sample_rgb, label_rgb, _ , _ = next(iter(test_dataloader_rgb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WejYgg_HyHhJ"
      },
      "source": [
        "## Test:\r\n",
        "\r\n",
        "#Test IR Model:\r\n",
        "print('TEST: Model IR')\r\n",
        "pred_ir = test_model(model_ir, sample_ir, label_ir)\r\n",
        "\r\n",
        "print('#########################################################################')\r\n",
        "\r\n",
        "#Test RGB Model:\r\n",
        "print('TEST: Model RGB')\r\n",
        "pred_rgb = test_model(model_rgb, sample_rgb, label_rgb)\r\n",
        "\r\n",
        "print('#########################################################################')\r\n",
        "\r\n",
        "#Test Combined:\r\n",
        "print('TEST: Combined IR + RGB')\r\n",
        "pred_combined = test_combined(model_ir, model_rgb, sample_ir, sample_rgb, label_ir, label_rgb)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWafZMq8j8KS"
      },
      "source": [
        "### Evaluate Models on whole apple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3w_5uz2j7af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfbeb6f-49e5-4ff1-f0d4-5de2909f0ff7"
      },
      "source": [
        "# Evaluation model for a single apple due to combining IR & RGB metrics\n",
        "\n",
        "pred_eval = pred_combined   #choose pred_ir, pred_rgb, or pred_combined\n",
        "\n",
        "## Apple 2:\n",
        "# Count number of predicted defects\n",
        "number_defects = len(pred_eval[:28]) - np.sum(pred_eval[:28])\n",
        "defect_threshold = 5\n",
        "\n",
        "if (number_defects < defect_threshold):\n",
        "  apple_class = \"sound\"\n",
        "else:\n",
        "  apple_class = \"bad\"\n",
        "\n",
        "print(\"Apple 2 was classified as \", apple_class, \" because \", number_defects, \" defects were detected!\")\n",
        "\n",
        "## Apple 30:\n",
        "# Count number of predicted defects\n",
        "number_defects = len(pred_eval[28:]) - np.sum(pred_eval[28:])\n",
        "defect_threshold = 5\n",
        "\n",
        "if (number_defects < defect_threshold):\n",
        "  apple_class = \"sound\"\n",
        "else:\n",
        "  apple_class = \"bad\"\n",
        "\n",
        "print(\"Apple 30 was classified as \", apple_class, \" because \", number_defects, \" defects were detected!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple 2 was classified as  bad  because  15  defects were detected!\n",
            "Apple 30 was classified as  bad  because  12  defects were detected!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23Toz8vI7Gu"
      },
      "source": [
        "### Save Model (State Dict)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOeYopHTkhvJ"
      },
      "source": [
        "## Manually save model (state-dict):\n",
        "\n",
        "#Save IR Model:\n",
        "torch.save(model_ir.state_dict(), os.path.join(model_dir, 'ir/', 'model_ir_new.pth'))\n",
        "\n",
        "#Save RGB Model:\n",
        "torch.save(model_rgb.state_dict(), os.path.join(model_dir, 'rgb/', 'model_rgb_new.pth'))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}